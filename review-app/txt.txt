import os
import pandas as pd
import datetime
import threading
import time
import boto3
from flask import Flask, request, jsonify, render_template
from minio import Minio
from minio.error import S3Error
import psycopg2

# Initialize Flask App
app = Flask(__name__, template_folder="/app/templates")

# MinIO Configuration
MINIO_ENDPOINT = "minio-service:9000"
MINIO_ACCESS_KEY = "minioadmin"
MINIO_SECRET_KEY = "miniosecret123"
BUCKET_NAME = "csv-uploads"

# AWS CloudWatch Configuration
CLOUDWATCH_NAMESPACE = "CSVProcessing"
cloudwatch_client = boto3.client("cloudwatch", region_name="us-east-2")

# Database Configuration
DB_HOST = "review-db-service"
DB_NAME = "reviewdb"
DB_USER = "postgres"
DB_PASSWORD = "postgres_password"

# Initialize MinIO Client
minio_client = Minio(
    MINIO_ENDPOINT,
    access_key=MINIO_ACCESS_KEY,
    secret_key=MINIO_SECRET_KEY,
    secure=False
)

# Ensure MinIO Bucket Exists
try:
    if not minio_client.bucket_exists(BUCKET_NAME):
        minio_client.make_bucket(BUCKET_NAME)
        print(f"‚úÖ MinIO Bucket Created: {BUCKET_NAME}")
    else:
        print(f"‚úÖ MinIO Bucket Exists: {BUCKET_NAME}")
except S3Error as e:
    print(f"‚ö†Ô∏è MinIO Error: {e}")

# Connect to Database
def connect_db():
    """Establishes a connection to PostgreSQL"""
    try:
        conn = psycopg2.connect(
            database=DB_NAME,
            user=DB_USER,
            password=DB_PASSWORD,
            host=DB_HOST,
            port="5432"
        )
        print("‚úÖ Database connected successfully")
        return conn
    except Exception as e:
        print(f"‚ö†Ô∏è Database connection failed: {e}")
        return None

conn = connect_db()
cur = conn.cursor() if conn else None

# Flask Routes
@app.route("/")
def index():
    """Renders the main CSV upload form and data table"""
    return render_template("review_form.html")

@app.route("/admin")
def admin():
    """Renders the Admin Panel for CSV Upload"""
    return render_template("admin_dashboard.html")

@app.route("/upload-csv", methods=["POST"])
def upload_csv():
    """Handles CSV uploads, stores them in MinIO, and auto-syncs to PostgreSQL"""
    if 'file' not in request.files:
        return jsonify({"error": "No file part"}), 400

    file = request.files['file']
    
    if file.filename == '':
        return jsonify({"error": "No selected file"}), 400

    file_path = f"/tmp/{file.filename}"
    file.save(file_path)  # Save locally before upload

    print(f"üîç Checking local file: {file_path}")
    if not os.path.exists(file_path):
        print(f"‚ö†Ô∏è Local file {file_path} does not exist.")
        return jsonify({"error": "File not found after saving locally"}), 500

    try:
        minio_client.fput_object(BUCKET_NAME, file.filename, file_path)
        print(f"‚úÖ File uploaded to MinIO: {file.filename}")

        # Auto-sync CSV data to PostgreSQL immediately after upload
        sync_csv_to_db(file.filename)

        return jsonify({"message": "File uploaded and synced successfully"}), 201
    except S3Error as e:
        print(f"‚ö†Ô∏è MinIO Upload Error: {e}")
        return jsonify({"error": "An error occurred while uploading the file"}), 500
def sync_csv_to_db(file_name):
    """Fetches a CSV from MinIO, processes it, and stores it in PostgreSQL without duplicates"""
    try:
        file_path = f"/tmp/{file_name}"
        minio_client.fget_object(BUCKET_NAME, file_name, file_path)
        df = pd.read_csv(file_path)

        table_name = "uploaded_data"

        # Ensure table exists
        column_definitions = ", ".join([f"{col} TEXT" for col in df.columns])
        cur.execute(f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                id SERIAL PRIMARY KEY,
                {column_definitions},
                upload_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        """)
        conn.commit()

        # ‚úÖ Prevent duplicate inserts by checking existing data
        existing_rows = set()
        cur.execute(f"SELECT company, product FROM {table_name};")
        for row in cur.fetchall():
            existing_rows.add((row[0], row[1]))

        # ‚úÖ Insert only new rows
        columns = ", ".join(df.columns)
        placeholders = ", ".join(["%s"] * len(df.columns))
        for _, row in df.iterrows():
            if (row["company"], row["product"]) not in existing_rows:  # Only insert if it's new
                cur.execute(f"""
                    INSERT INTO {table_name} ({columns}, upload_timestamp)
                    VALUES ({placeholders}, %s);
                """, tuple(row) + (datetime.datetime.now(),))
        conn.commit()

        print(f"‚úÖ {file_name} processed and stored in PostgreSQL without duplicates")

    except Exception as e:
        print(f"‚ö†Ô∏è Error syncing CSV to database: {e}")
        conn.rollback()


@app.route("/get-data", methods=["GET"])
def get_data():
    """Fetches all uploaded CSV data from the database"""
    try:
        table_name = "uploaded_data"
        print(f"üîç Fetching data from table: {table_name}")

        cur.execute(f"SELECT * FROM {table_name};")
        rows = cur.fetchall()

        if not rows:
            return jsonify({"error": "No data found"}), 404

        # Fetch column names dynamically
        columns = [desc[0] for desc in cur.description]
        data = [dict(zip(columns, row)) for row in rows]

        print(f"‚úÖ Data Retrieved: {data}")
        return jsonify(data)

    except Exception as e:
        print(f"‚ö†Ô∏è Error fetching data: {e}")
        return jsonify({"error": "Failed to fetch data"}), 500


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8000)
